<!DOCTYPE html>
<html lang="ko">
<head>
  <meta charset="utf-8">
  <title>Research | CAD & SoC Design Lab.</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <style>
    /* ===== 기본 레이아웃 ===== */
    body{
      font-family:"Helvetica Neue",Arial,sans-serif;
      margin:0;
      padding:40px 5%;
      line-height:1.6;
      color:#222;
    }
    .container{
      display:flex;
      gap:20px;
      max-width:1600px;
      margin:0 auto;
      padding:40px 5%;
    }
    /* --- 왼쪽 사이드바 --- */
    .sidebar{flex:0 0 260px;text-align:center}
    .sidebar img{width:180px;height:auto}
    .sidebar h1{font-size:28px;line-height:1.2;margin:0 0 20px}
    .sidebar p{font-size:14px;color:#555}
    /* --- 본문 영역 --- */
    .content{flex:1 1 0;max-width:700px}
    .content h2{margin-top:0}
    .content h3{margin:22px 0 6px}
    .content ul{margin:0 0 18px 18px}
    /* --- 상단 네비게이션 --- */
    .nav{margin:0 0 30px;padding:0;list-style:none;display:flex;gap:12px 25px;flex-wrap:wrap}
    .nav a{color:#006BB3;text-decoration:none}
    .nav a:hover{text-decoration:underline}
    /* ===== 연구 탭 버튼 ===== */
    .tab-box{
      display:grid;
      grid-template-columns:repeat(2,1fr); /* 1행 2열 */
      gap:8px;
      max-width:700px;
      margin-bottom:30px;
    }
    .tab-btn{
      padding:10px 0;
      border:1px solid #006BB3;
      text-align:center;
      font-weight:600;
      cursor:pointer;
      user-select:none;
      transition:.2s;
    }
    .tab-btn.active{background:#006BB3;color:#fff}
    /* 탭 콘텐츠 기본 숨김 → 선택된 것만 보임 */
    .tab-content{display:none}
    .tab-content.active{display:block}
    /* 이미지 100% 폭 */
    .res-img{width:100%;height:auto}
    a{color:#006BB3}

    .video-row{
      display:flex;
      justify-content:center;   /* 가운데 정렬 */
      gap:40px;                 /* 두 영상 사이 간격 */
      flex-wrap:wrap;
      margin-top:40px;
    }
    
    /* ▽ ② 비디오를 더 크게 – 폭을 최대 90 %, 800 px 까지 */
    .video-row video{
      width:90%;        /* 컨텐트 폭의 90 % */
      max-width:800px;  /* 너무 커지지 않도록 상한 */
      aspect-ratio:16/9;
      border:1px solid #ccc;
      border-radius:4px;
    }

    /* ▼ 화면이 좁을 때(예: 모바일) 레이아웃 전환 --------------- */
    @media (max-width:1200px){
      .container{              /* 가로 배치 → 세로 쌓기 */
        flex-direction:column; 
        padding:25px 5%;
      }
      .sidebar{                /* 폭 제약 해제 + 여백 */
        flex:0 0 auto;
        margin-bottom:30px;
      }
      .sidebar img{width:140px}  /* 로고 조금 작게(선택) */
      .content{max-width:100%}    /* 본문은 전체폭 사용 */
      .video-row video{width:100%}/* 비디오도 화면폭에 맞춰 */
    }
  
    /* 본문 문단을 양쪽 정렬로 */
    .content p{
      text-align:justify;      /* 양쪽 정렬 */
      text-justify:inter-word; /* 한-글/영문 모두 단어 기준으로 공백 조정 */
    }
  
  </style>
</head>

<body>
<div class="container">
  <!-- ===== 사이드바 ===== -->
  <aside class="sidebar">
    <h1>CAD &amp; SoC<br>Design Lab.</h1>
    <img src="./Image/CSDL.jpg" alt="Lab Logo">
    <img src="./Image/POSTECH.png" alt="POSTECH Logo">
    <p>CAD &amp; SoC Design Laboratory at POSTECH.</p>
    <p>This website is temporarily in use.</p>
  </aside>

  <!-- ===== 본문 ===== -->
  <main class="content">
    <!-- 상단 메뉴 -->
    <ul class="nav">
      <li><a href="index.html"><strong>Home</strong></a></li>
      <li><strong>Research</strong></li>
      <li><a href="Advisor.html"><strong>Advisor</strong></a></li>
      <li><a href="Members.html"><strong>Members</strong></a></li>
      <li><a href="Publications.html"><strong>Publications</strong></a></li>
    </ul>

    <!-- ===== 탭 버튼 (1행 2열) ===== -->
    <div class="tab-box">
      <div class="tab-btn active" data-target="pdopt">Physical Design Optimization</div>
      <div class="tab-btn"        data-target="dnn">Deep Learning Hardware</div>
    </div>

    <!-- ---------------- Physical Design Optimization ---------------- -->
    <div id="pdopt" class="tab-content active">

      <div class="video-row">
        <video controls preload="metadata" width="720" style="display:block; margin:20px auto;">
          <source src="./Video/3_EDA.mp4" type="video/mp4">
          Your browser does not support MP4 video.
        </video>
      </div>

      <h2>Physical Design Optimization</h2>

      <p>In the field of Electronic Design Automation (EDA) for Very-Large-Scale Integration (VLSI), physical design optimization focuses on power consumption, circuit performance, and chip area (PPA). 
      EDA tools automate complex tasks in the design process, such as circuit simulation, layout design, and verification, significantly enhancing efficiency and accuracy.
      Advanced silicon scaling fabrication technologies, including 3D ICs, FinFET, and GAA, produce denser chips with improved PPA. 
      However, the complexity of designs, which now contain various modules on a single chip (SoC-level), has exponentially increased.
      Recent advancements in EDA encompass efficient design methodologies for emerging technology processes and the incorporation of AI, including machine learning (ML) and reinforcement learning (RL).</p>
      
      <p>Our lab (CSDL) researches EDA to improve PPA using both heuristic methods and AI. Our interests span all steps of physical design, analog circuit optimization, standard cell generation, and design-technology co-optimization (DTCO).
      We also focus on prediction, optimization and generation using ML and RL for each step of physical design and design space optimization (DSO).
      Additionally, we actively participate in the ISPD contest and ICCAD contest, which are published annually by the International Symposium of Physical Design (ISPD) every summer and the International Conference of Computer-Aided Design (ICCAD) every winter, respectively.
      Our current research is categorized as below.</p>

      <ul>
        <li><a href="https://github.com/CSDL-postech/Topic_Introduction/tree/main/Physical_Design">Physical Design</a></li>
        <li><a href="https://github.com/CSDL-postech/Topic_Introduction/tree/main/Analog_Circuit_Opt">Analog Circuit Optimization</a></li>
        <li><a href="https://github.com/CSDL-postech/Topic_Introduction/tree/main/DTCO">Design-Technology Co-Optimization</a></li>
        <li><a href="https://github.com/CSDL-postech/Topic_Introduction/tree/main/STD_Cell_Generation">Standard Cell Generation</a></li>
        <li><a href="https://github.com/CSDL-postech/Topic_Introduction/tree/main/ML_Prediction_and_Opt">ML-based Prediction &amp; Optimization</a></li>
        <li><a href="https://github.com/CSDL-postech/Topic_Introduction/tree/main/ML_Generation">ML-based Generation</a></li>
        <li><a href="https://github.com/CSDL-postech/Topic_Introduction/tree/main/RL_Opt">RL-based Optimization</a></li>
        <li><a href="https://github.com/CSDL-postech/Topic_Introduction/tree/main/Design_Space_Opt">Design Space Optimization</a></li>
      </ul>
    </div><!-- /#pdopt -->

    <!-- ---------------- Deep Learning Hardware ---------------- -->
    <div id="dnn" class="tab-content">
      
      <div class="video-row">
        <video controls preload="metadata" width="720" style="display:block; margin:20px auto;">
          <source src="./Video/4_DLHW.mp4" type="video/mp4">
          Your browser does not support MP4 video.
        </video>
      </div>
      
      <h2>Deep Learning Hardware</h2>

      <p>Deep Neural Networks (DNNs) have proven to show superior performance compared to conventional algorithms in the field of Computer Vision (CV) and Natural Language Processing (NLP).
      However, due to its computational complexity, high energy consumption and memory demand, both academia and industry find it difficult to port DNN-based tasks on traditional CPUs and GPUs. 
      To tackle this problem, software-based model compression methods were proposed to increase energy and memory efficiency of DNNs. Also, engineers developed domain-specific hardware to efficiently accelerate the heavy DNN computation.</p>
      
      <p>Our lab also studies and develops domain-specific hardware accelerators and model compression techniques for efficient deployment of DNNs. 
      We target acceleration and compression of both traditional Convolution-based Neural Networks (CNNs) and attention-based transformers. 
      Our current research is the optimization of DNN training, where we aim to minimize memory footprint and training latency for effective deployment of DNNs for on-device training (edge computing).
      We also study model compression techniques for attention-based transformers, such as Vision Transformers (ViT) and Large-Language Models (LLMs).</p>

      <img class="res-img" src="./Image/Edge_Cloud.png"  alt="Edge vs Cloud">
      <img class="res-img" src="./Image/HW_Series.png"   alt="DNN Processing Units">
    </div><!-- /#dnn -->

  </main><!-- /.content -->
</div><!-- /.container -->

<!-- ===== 탭 스크립트 ===== -->
<script>
document.querySelectorAll('.tab-btn').forEach(btn=>{
  btn.addEventListener('click', ()=>{
    // 버튼 active 상태
    document.querySelectorAll('.tab-btn').forEach(b=>b.classList.remove('active'));
    btn.classList.add('active');
    // 콘텐츠 토글
    const tgt = btn.dataset.target;
    document.querySelectorAll('.tab-content').forEach(box=>{
      box.classList.toggle('active', box.id === tgt);
    });
  });
});
</script>

<footer style="text-align:center;font-size:13px;margin-top:40px;color:#777;">
  Hosted on GitHub Pages — Theme by orderedlist
</footer>
</body>
</html>
