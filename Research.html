<!DOCTYPE html>
<html lang="ko">
<head>
  <meta charset="utf-8">
  <title>Members | CAD & SoC Design Lab.</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- ===== 기본 스타일(기존 index.html과 동일) ===== -->
  <style>
    body{
      font-family: "Helvetica Neue", Arial, sans-serif;
      margin:0;
      padding:40px 5%;
      line-height:1.6;
      color:#222;
    }
    .container{
      display:flex;
      gap:60px;
      max-width:1200px;
      margin:0 auto;
    }
    /* --- 사이드바 --- */
    .sidebar{flex:0 0 260px;text-align:center}
    .sidebar img{width:180px;height:auto}
    .sidebar h1{font-size:28px;line-height:1.2;margin:0 0 20px}
    .sidebar p{font-size:14px;color:#555}

    /* --- 본문 --- */
    .content{flex:1 1 0;max-width:700px}
    .content h2{margin-top:0}
    .content h3{margin:22px 0 6px}
    .content ul{margin:0 0 18px 18px}
    /* 네비게이션(선택) */
    .nav{margin:0 0 30px;padding:0;list-style:none;display:flex;gap:12px 25px;flex-wrap:wrap}
    .nav a{color:#006BB3;text-decoration:none}
    .nav a:hover{text-decoration:underline}
    a{color:#006BB3}
  </style>
</head>
<body>
  <div class="container">
    <!-- ========== 왼쪽 사이드바 ========== -->
    <aside class="sidebar">
      <h1>CAD &amp; SoC<br>Design Lab.</h1>
      <img src="./Image/CSDL.jpg" alt="Lab Logo">
      <p>CAD &amp; SoC Design Laboratory at POSTECH.</p>
      <p>This website is temporarily in use.</p>
    </aside>

    <!-- ========== 오른쪽 본문 ========== -->
    <main class="content">
      <!-- (선택) 상단 메뉴 -->
      <ul class="nav">
        <li><a href="index.html">Home</a></li>
        <li><strong>Research</strong></li>
        <li><a href="./Advisor.html">Advisor</a></li>
        <li><a href="./Members.html">Members</a></li>   <!-- 여기 수정 -->
        <li><a href="./Publications.html">Publications</a></li>
      </ul>

      <h2>Physical Design Optimization</h2>
      <ul>
        <li>In the field of computer-aided design for VLSI(Very Large Scale Integrated circuits), physical design optimization is concerned with power consumption, chip area and performance of circuits.
        For a long time, physical design optimization was not an issue. However, because of improved fabrication technology, recently circuit designs are becoming much denser from PCB (Printed Circuit Board) based system to SoC (System-on-Chip) level.
        As SoC level chip contains various modules in a single chip, design complexity become exponentially increased and this makes big issue for achieving good performance.
        </li>
      
        
        <li>Our lab (SoCDL) research physical design optimization as based on participating ISPD Contest, ICCAD Contest. Each contest published by each conference ISPD(International Symposium of Physical Design) and ICCAD(International Conference of Computer-Aided Design).
        Each contest is held every summer and winter season annually. Various research papers are published on ISPD and ICCAD based on contest result. Our recent paper was published on DAC(Design Automation Conference) 2017 based on ICCAD 2015 contest result.
        </li>
      </ul>


      <h2>Deep Learning Hardware</h2>
      <ul>
      <li>Deep Neural Networks (DNNs) have proven to show superior performance compared to conventional algorithms in the field of Computer Vision (CV) and Natural Language Processing (NLP).
      However, due to its computational complexity, high energy consumption and memory demand, both academia and industry find it difficult to port DNN-based tasks on traditional CPUs and GPUs. 
      To tackle this problem, software-based model compression methods were proposed to increase energy and memory efficiency of DNNs. Also, engineers developed domain-specific hardware to efficiently accelerate the heavy DNN computation. 
      </li>

      <li>Our lab also studies and develops domain-specific hardware accelerators and model compression techniques for efficient deployment of DNNs. 
      We target acceleration and compression of both traditional Convolution-based Neural Networks (CNNs) and attention-based transformers. 
      Our current research is the optimization of DNN training, where we aim to minimize memory footprint and training latency for effective deployment of DNNs for on-device training (edge computing).
      We also study model compression techniques for attention-based transformers, such as Vision Transformers (ViT) and Large-Language Models (LLMs).
      </li>
      </ul>

    </main>
  </div>

  <footer style="text-align:center;font-size:13px;margin-top:40px;color:#777;">
    Hosted on GitHub Pages — Theme by orderedlist
  </footer>
</body>
</html>
